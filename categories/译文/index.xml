<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>译文 on ServiceMesher</title>
    <link>https://servicemesher.github.io/categories/%E8%AF%91%E6%96%87/</link>
    <description>Recent content in 译文 on ServiceMesher</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Wed, 06 Jun 2018 18:30:53 +0800</lastBuildDate>
    
	<atom:link href="https://servicemesher.github.io/categories/%E8%AF%91%E6%96%87/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Istio v1aplha3 routing API介绍</title>
      <link>https://servicemesher.github.io/blog/introducing-the-istio-v1alpha3-routing-api/</link>
      <pubDate>Wed, 06 Jun 2018 18:30:53 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/introducing-the-istio-v1alpha3-routing-api/</guid>
      <description>原文链接：https://istio.io/blog/2018/v1alpha3-routing
作者：Frank Budinsky (IBM) and Shriram Rajagopalan (VMware)
译者：赵化冰
校对：宋净超
 到目前为止，Istio提供了一个简单的API来进行流量管理，该API包括了四种资源：RouteRule、DestinationPolicy、EgressRule和Ingress（直接使用了Kubernets的Ingress资源）。借助此API，用户可以轻松管理Istio服务网格中的流量。该API允许用户将请求路由到特定版本的服务，为弹性测试注入延迟和失败，添加超时和断路器等等，所有这些功能都不必更改应用程序本身的代码。
虽然目前API的功能已被证明是Istio非常引人注目的一部分，但也有一些用户反馈该API确实有一些缺点，尤其是在使用它来管理包含数千个服务的大型应用，以及使用HTTP以外的协议时。 此外，使用Kubernetes Ingress资源来配置外部流量的方式已被证明不能满足需求。
为了解决上述缺陷和其他的一些问题，Istio引入了新的流量管理API v1alpha3，新版本的API将完全取代之前的API。 尽管v1alpha3和之前的模型在本质上是基本相同的，但旧版API并不向后兼容，基于旧API的模型需要进行手动转换。Istio的后续版本中会提供一个新旧模型的转换工具。
为了证明该非兼容性升级的必要性，v1alpha3 API经历了漫长而艰苦的社区评估过程，以希望新的API能够大幅改进，并经得起时间的考验。在本文中，我们将介绍新的配置模型，并试图解释其后面的一些动机和设计原则。
设计原则 路由模型的重构过程中遵循了一些关键的设计原则：
 除支持声明式（意图）配置外，也支持显式指定模型依赖的基础设施。例如，除了配置入口网关（的功能特性）之外，负责实现入口网关功能的组件（Controller）也可以在模型指定。 编写模型时应该“生产者导向”和“以Host为中心”，而不是通过组合多个规则来编写模型。 例如，所有与特定Host关联的规则被配置在一起，而不是单独配置。 将路由与路由后行为清晰分开。  v1alpha3中的配置资源 在一个典型的网格中，通常有一个或多个用于终止外部TLS链接，将流量引入网格的负载均衡器（我们称之为gateway）。 然后流量通过sidecar网关（sidecar gateway）流经内部服务。应用程序使用外部服务的情况也很常见（例如访问Google Maps API），一些情况下，这些外部服务可能被直接调用；但在某些部署中，网格中所有访问外部服务的流量可能被要求强制通过专用的出口网关（Egress gateway）。 下图描绘了网关在网格中的使用情况。
考虑到上述因素，v1alpha3引入了以下这些新的配置资源来控制进入网格、网格内部和离开网格的流量路由。
 Gateway VirtualService DestinationRule ServiceEntry  VirtualService、DestinationRule和ServiceEntry分别替换了原API中的RouteRule、DestinationPolicy和EgressRule。Gateway是一个独立于平台的抽象，用于对流入专用中间设备的流量进行建模。
下图描述了跨多个配置资源的控制流程。 Gateway Gateway用于为HTTP/TCP流量配置负载均衡器，并不管该负载均衡器将在哪里运行。网格中可以存在任意数量的Gateway，并且多个不同的Gateway实现可以共存。实际上，通过在配置中指定一组工作负载（Pod）标签，可以将Gateway配置绑定到特定的工作负载，从而允许用户通过编写简单的Gateway Controller来重用现成的网络设备。
对于入口流量管理，您可能会问：为什么不直接使用Kubernetes Ingress API？原因是Ingress API无法表达Istio的路由需求。Ingress试图在不同的HTTP代理之间取一个公共的交集，因此只能支持最基本的HTTP路由，最终导致需要将代理的其他高级功能放入到注解（annotation）中，而注解的方式在多个代理之间是不兼容的，无法移植。
Istio Gateway 通过将L4-L6配置与L7配置分离的方式克服了Ingress的这些缺点。Gateway只用于配置L4-L6功能（例如，对外公开的端口、TLS配置），所有主流的L7代理均以统一的方式实现了这些功能。 然后，通过在Gateway上绑定VirtualService的方式，可以使用标准的Istio规则来控制进入Gateway的HTTP和TCP流量。
例如，下面这个简单的Gateway配置了一个Load Balancer，以允许访问host bookinfo.com的https外部流量入mesh中：
apiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata: name: bookinfo-gateway spec: servers: - port: number: 443 name: https protocol: HTTPS hosts: - bookinfo.</description>
    </item>
    
    <item>
      <title>使用 Istio 为微服务提供高级流量管理和请求跟踪功能</title>
      <link>https://servicemesher.github.io/blog/manage-microservices-traffic-using-istio/</link>
      <pubDate>Wed, 06 Jun 2018 15:32:26 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/manage-microservices-traffic-using-istio/</guid>
      <description>原文地址：https://developer.ibm.com/code/patterns/manage-microservices-traffic-using-istio/
作者：IBM
译者：Jimmy Song
 说明 开发人员正在摆脱大型单体应用的束缚，转而采用小巧而专一的微服务，以加速软件开发并加强系统弹性。为了满足这个新生态的需求，开发人员需要为部署的微服务创建一个具有负载均衡、高级流量管理、请求跟踪和连接功能的服务网络。
概述 如果您花时间开发过应用程序，那么有件事情您肯定明白：单体应用正成为过去。当今的应用程序都是关于服务发现、注册、路由和连接。这给微服务的开发和运维人员提出了新的挑战。
如果您的服务网格在规模和复杂性上不断增长，您可能想知道如何理解和管理服务网格。我们也遇到了同样的问题：如何使这些越来越多的微服务能够彼此连接、负载均衡并提供基于角色的路由？如何在这些微服务上启用传出流量并测试金丝雀部署？仅仅创建一个独立的应用程序还不够，所以我们该如何管理微服务的复杂性呢？
Istio 是 IBM、Google 和 Lyft 合作创建的项目，旨在帮助您应对这些挑战。Istio 是一种开放技术，它为开发人员提供了一种这样的方式：无论是什么平台、来源或供应商，微服务之间都可以无缝连接，服务网格会替您管理和保护微服务。在下面的开发之旅中，您将了解如何通过 Istio 基于容器的 sidecar 架构提供复杂的流量管理控制功能，它既可用于微服务之间的互通，也可用于入口和出口流量。您还将了解如何监控和收集请求跟踪信息，以便更好地了解您的应用流量。此次开发者之旅对于所有使用微服务架构的开发人员来说都是理想之选。
流程  用户在 Kubernetes 上部署其配置的应用程序。应用程序 BookInfo 由四个微服务组成。该应用中的微服务使用不同的语言编写——Python、Java、Ruby 和 Node.js。Reivew 微服务使用 Java 编写，有三个不同的版本。 为了使应用程序能够利用 Istio 的功能，用户将向微服务中注入 Istio envoy。Envoy 使用 sidecar 的方式部署在微服务中。将 Envoy 注入到微服务中也意味着使用 Envoy sidecar 管理该服务的所有入口和出口流量。然后用户访问运行在 Istio 上的应用程序。 应用程序部署完成后，用户可以为示例应用程序配置 Istio 的高级功能。要启用流量管理，用户可以根据权重和 HTTP 标头修改应用的服务路由。在该阶段，Review 微服务的 v1 版本和 v3 版本各获得 50％ 的流量；v2 版本仅对特定用户启用。 用户配置服务的访问控制。为了拒绝来自 v3 版本的 Review 微服务的所有流量对 Rating 微服务的访问，用户需要创建 Mixer 规则。 完成应用程序的部署和配置后，用户可以启用遥测和日志收集功能。为了收集监控指标和日志，用户需要配置 Istio Mixer 并安装所需的 Istio 附件 Prometheus 和 Grafana。要收集 trace span，用户需要安装并配置 Zipkin 附件。 用户为 Bookinfo 创建一个外部数据源；例如 IBM Cloud 中的 Compose for MySQL 数据库。 原始示例 BookInfo 应用程序中的三个微服务——Details、Ratings 和 Review ，已修改为使用 MySQL 数据库。要连接到 MySQL 数据库，需要在 Details 微服务中添加了一个 MySQL Ruby gem；向 Ratings Node微服务中添加 MySQL 模块。将 mysql-connector-java 依赖项添加到 Reviews 微服务 v1、v2 和 v3 版本中。 用户部署应用程序并启用具有出口流量的 Envoy 代理。Envoy 代理作为 sidecar 跟每个微服务部署在一起。Envoy sidecar 将管理该服务中所有流入和流出的流量。当前情况下，由于 Envoy 仅支持 http/https 协议，因此通过提供 MySQL 的 IP 地址范围，代理配置将不会拦截到 MySQL 连接的流量。当应用程序启动后，用户可以使用 IP 和节点端口访问应用程序。  查看该示例中的代码：https://github.</description>
    </item>
    
    <item>
      <title>服务网格：8种方式简化微服务部署</title>
      <link>https://servicemesher.github.io/blog/8-ways-a-service-mesh-eases-microservices-deployment/</link>
      <pubDate>Tue, 05 Jun 2018 19:12:42 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/8-ways-a-service-mesh-eases-microservices-deployment/</guid>
      <description>原文地址：https://thenewstack.io/8-ways-a-service-mesh-eases-microservices-deployment/
作者：Robert Whiteley
译者：Grace
 基于微服务的架构是未来的趋势，但是实现这种架构会面临许多困难。现代应用架构远比过去的架构复杂，因此实现微服务架构将会带来了一系列特殊的挑战，而服务网格可以帮我们解决很多问题。
最近一段时间，管理者不再专注于调试单个应用程序服务器，相反，现代系统就像是一群牛，研究整体的行为远比单个的服务器更有意义，分布式系统就是一个典型。
微服务是一种分布式架构，目的在于通过不断调整自身以适应当前流量状况的变化，例如，有一组处理客户端请求路由的容器，改变这组容器，反过来也意味着路由表在不断变化，由此反映了应用程序端点的变化位置。与此同时，在任何架构体系中都会有过去的遗留物，从必须使用单个大型数据库服务器的应用程序到捆绑API以使其看起来是以服务为重点的遗留系统。
而服务网格是当前最先进的微服务模式。它建立在容器以及容器编排之上，配有处理内部服务通信的专用控制面。它负责协调分布式网格的微服务所需的安全性，路由，身份验证，授权和类似功能，服务网格将这些功能从应用程序（或应用程序的服务组件）中剥离出来作为可编程的基础组件。虽然不是所有的公司都需要如此复杂的服务网格（尽管这些公司大部分都运行着成百上千的服务），但服务网格正迅速成为那些希望运行生产级微服务的公司的默认架构。
以下是八种实现服务网格的方法，可以帮助您平滑过渡到微服务。
 改进微服务的消息处理机制。服务网格确保你能监控到整个架构层，不仅可以跟踪到网络中的服务器地址，还可以跟踪到传达服务器地址信息的消息。例如，你可能想要跟踪“失败”消息，但这些消息在传统云架构中通常会丢失。服务网格的好处是既可以确保消息的传递，又会在消息未到达目的地时返回错误信息。
 利用与传统应用程序相同的运维方式。对于企业级网络来说，可定制性和灵活性是最重要的。服务网格是为适应现代分布式应用程序而设计的。但是底层的技术如入口控制器，负载均衡器，以及代理都和传统单体应用的数据层面的技术相同。在实现服务网格的过程中，组织可以利用到与运营现代、基于软件的应用程序交付基础设施相同的技术与技能。
 灵活使用多种云服务。服务网格解决了现代应用的云网络问题。支撑起服务网格的数据平面和控制平面的技术独立于任何特定架构，因此它们可以在无论是裸机，容器还是虚拟机的公有或私有的架构上运行。这种灵活特性甚至允许服务网格处理未来的应用程序架构，从而发挥其规模化、全球复制以及深层性能调节等优势。您的服务网格将成为运作模式化云架构场景下，一切潜在优势的实现保障。
 提高对微服务的可见性。分布式系统的指标对于我们而言就像是一个黑盒子，而网格服务为我们提供了一种更深入观察分布式系统的指标的途径。它会随时间收集性能指标，为团队提供服务可用性的长期指标。这为操作员提供了一种观察服务可靠性和性能的方式，使他们能够逐步优化系统。
 更高效的运维以及更有效的执行SLA（服务等级协议）。服务网格提供的追踪功能对调试和故障排除至关重要，与此同时，它也确保服务执行了服务等级协议（SLA）。服务网格执行了很多任务，包括执行策略以及追踪查看这些策略是否被满足。它为管理者提供了一个可以在网络层实施云应用管理和策略的场所。
 简化微服务实现。服务网格的另一大优点是可以轻松部署它们。过去的解决方案要求开发人员将服务内功能编码到每个微服务中。这需要重写应用程序并在不同的编程语言中维护各种库。而服务网格帮开发人员抽象了这些事务。开发人员可以简单地调用必要的消息传递和服务发现功能就可以轻松的部署它们，而微服务的源码只用包含业务逻辑相关的代码。
 加快新服务的上线时间。过去的库解决方案，如Finagle、Hystrix和Stubby，需要开发人员长时间的介入并且迫使开发人员将冗余功能编码到每一个服务中。另一个更简单的方法是在每个微服务中放置一个sidecar代理并将它们连接在一起，这正是服务网格所擅长的，因此未来将会有更多的云应用选择服务网格架构。简而言之，服务网格保证了开发者的生产力，使他们能够更快地将更多的服务推向市场。
 保障服务间的通信安全。服务之间通信有可能跨云，跨数据中心，或者跨大陆，而服务网格保障了这些通信的安全，它封装了所有的通信，并且在控制器层面协调这些通信，通过管道内加密，联系人策略和服务权限解决了安全问题。
  转载自：https://mp.weixin.qq.com/s/Q-k0BtUz2U4bWiXXdeaw7g</description>
    </item>
    
    <item>
      <title>服务网格之路</title>
      <link>https://servicemesher.github.io/blog/the-path-to-service-mesh/</link>
      <pubDate>Mon, 04 Jun 2018 15:55:08 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/the-path-to-service-mesh/</guid>
      <description>原文链接：https://blog.aspenmesh.io/blog/2018/03/the-path-to-service-mesh/
作者：aspenmesh.io
译者：卢宇亮
 当我们谈论服务网格的时候，有几个问题经常被提及。这些问题的范围覆盖从简单的了解服务网格的历史，到产品和架构相关的比较深入的技术问题。
为了回答这些问题，通过 Aspen Mesh 之旅，我们带来三个主题的系列博文来讨论我们为什么选择了 Istio 。
作为开始，我将重点讨论我最经常被问到的问题之一：
为什么你选择服务网格，是什么原因促使你这样做？
LineRate ：高性能负载均衡软件 这个旅程起源于来自 Boulder 的初创公司 LineRate ，该公司在2013年被 F5 Networks 公司收购。 LineRate 除了是我曾经有幸参与的最聪明、最有才华的工程团队，还是一款轻量级高性能 L7 软件代理。当我说高性能时，我正在谈论的是如何将5年前在数据中心已经存在的服务器，变成一个高性能20+ Gbps 200,000+ HTTP 请求每秒的全功能负载。
虽然性能本身是引入注目的并为我们的客户打开了大门，但是我们的出发点在于客户期望付费的是容量，而不是硬件。这种见解是 LineRate 的核心价值主张。这个简单的概念将使我们的客户能够改变他们在应用之前使用和部署负载均衡的方式。
为了满足这个需求，我们交付了一种产品和商业模式，使我们的客户能够基于 COTS （可在市场上买到的）硬件按需多次复制他们的软件，从而不管部署多少实例都可以获得峰值性能。如果客户需要更多的容量，他们只需要简单的升级其订购层并部署更多的产品副本，直到达到他们许可证允许的带宽，请求速率或者交易速率。
这很有吸引力，我们也取得了一些成就，但是很快我们有了新的想法&amp;hellip;&amp;hellip;
效率优于性能 对于我们而言，应用架构正在发生变化，而客户的价值曲线随之变化的趋势也变得明显。我们在与资深团队沟通的过程中注意到，他们讨论的是诸如效率，敏捷，速度，印迹和横向扩展这类的概念。同时我们也开始听到这些领域的创新者开始采用Docker的新技术，以及它将如何改变应用和服务交付的方式。
我们与这些团队交流的越多，思考我们如何开发自己的内部应用程序，我们就越意识到转变正在发生。团队从根本上改变他们交付应用的方式，结果是我们的客户开始更少的关注原始性能而是更多地关心分布式代理。这些转变还有更多地收益，包含减少应用的故障域，增加部署的灵活性和赋予应用将负载和网络作为配置管理的能力。
与此同时容器和容器编排引擎也开始登上舞台，因此我们开始致力于通过一个新的控制面板以容器的方式交付 LineRate 的产品，并深入的思考人们未来会如何使用这些新技术来交付应用。
这些发生在2015的早期讨论促使我们思考未来应用交付将会如何&amp;hellip;&amp;hellip;
与时俱进的想法 随着我们对于未来应用交付方式的思考，我们开始关注云原声分布式应用领域中有关策略和网络服务的概念。尽管我们仍然有很多不同的优先级项目，改变应用蓝图，云原生应用和基于DevOps交付模式的想法始终在我们思想的最前端。
在这个领域将会有一个新的市场。
我们设计了许多项目，但由于种种原因未能成功。我们亲切的称这些项目为 v1.0 ，v1.5 和 v2.0 。每个项目都有一种解决分布式应用架构（微服务）挑战的独特技术。
我们尽最大可能去思考。下一个应用交付控制架构（ ADC ):一个完全与 API 驱动的控制面板和一个分离的数据面板。数据面板可能来自云你能够设想到的任意一种形式：靠近微服务的专用硬件，商用软件，或者云原生组件（就像服务网格）。这种无限可扩展的架构可以实现优雅的平衡，能够完美的工作于任意规模的任意组织的任意一种工作。很有野心吧？我们陷入了为客户提供所有东西的陷阱。
接下来，我们在“1.5”中完善了我们的方法，我们决定开发一种策略语言&amp;hellip;&amp;hellip; 关键是要定义开源的策略接口并将它无缝地连接到完成工作的数据路径。在一个真正开放的平台中，其中一些数据路径也是开源的。但是仍然有很多发展中的事情没有一步到位；事后看来，其中一些事情已经到来了&amp;hellip;&amp;hellip; 市场还没有到来，加上我们在开源方面也没有专业知识，于是我们在描述我们在做什么以及为什么时遇到了麻烦。
但是想法仍然在我们的脑海中燃烧，而我们也没有放弃&amp;hellip;&amp;hellip;
在 2.0 版本，我们设计了一个帮助希望开始容器之旅的 F5 的用户的计划。技术是新的，而市场也刚刚开始走向成熟，我们决定用户将会通过三步开启他们的微服务之旅。</description>
    </item>
    
    <item>
      <title>使用 Minikube-in-a-Container 和 Jenkins 构建 Istio</title>
      <link>https://servicemesher.github.io/blog/building-istio-with-minikube-in-a-container-and-jenkins/</link>
      <pubDate>Mon, 04 Jun 2018 11:33:16 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/building-istio-with-minikube-in-a-container-and-jenkins/</guid>
      <description>原文链接：https://blog.aspenmesh.io/blog/2018/01/building-istio-with-minikube-in-a-container-and-jenkins/
作者：Andrew Jenkins
译者：戴佳顺
 AspenMesh提供一种Istio的分布式架构支持，这意味着即使与上游Istio项目无关，我们也需要能够测试和修复Bug。为此我们已开发构建了我们自己的打包和测试基础架构方案。如果你对Istio的CI（持续集成）也感兴趣，请参考我们已经投入使用，可能有用但还没有提交给Circle CI或GKE的组件。
这篇文章描述的是我们如何制作一个新的Minikube-in-a-Container容器和使用Jenkins Pipeline来构建和测试Istio的流程脚本。如果你觉得有必要，你可以通过docker run上运行minikube容器，然后在容器中部署功能性的kubernetes集群，不需要使用时可随时删除。Jenkins bits现在可帮助你构建Istio，也可以作为初始环境，以便在容器内构建容器。
Minikube-in-a-container 这部分描述了我们如何构建一个可以在构建过程中用来运行Istio冒烟测试的Minikube-in-a-container镜像。我们最初不是这么想的，我们最初使用本地localkube环境。我们不能让它在特定环境外工作，我们认为这是由于localkube和minikube之间有一点差异导致的。所以这是一个作为我们修复它使它能正常工作的记录。我们还添加了一些额外选项和工具，以便在生成的容器中使用Istio。这没有什么太多花样，但如果你要做类似的事情，我们希望它给你启发。
Minikube可能对你来说是一个可以在随身携带的笔记本上通过虚机运行自己kubernetes集群的非常熟悉的项目。这种方法非常方便，但在某些情况下（比如不提供嵌套虚拟化的云提供商），你就不能或者不希望基于虚机来完成了。由于docker现在可以运行在docker内部，我们决定尝试在docker容器内制作我们自己的kubernetes集群。一个非持久性的kubernetes容器很容易启动，也可进行一些测试，并在完成后进行删除。同时这也非常适合持续集成。
在我们的模型方案中，Kubernetes集群创建子docker容器（而不是Jérôme Petazzoni所提到的兄弟容器方案）。我们是故意这样做的，宁愿隔离子容器，而不是共享Docker构建的缓存。但是你应该在将你应用改造为DinD（docker in docker）之前阅读Jérôme的文章，也许DooD（在docker out of docker）是对你而言更好的方案。这篇文章供你参考。我们避免架构“变得更坏”的同时，对看起来“坏”和“丑”部分也应进行避免。
当你启动docker容器时，会要求docker在OS内核中创建和设置一些命名空间（namespaces），然后在通过这些命名空间启动你的容器。命名空间像一个沙箱：当你在命名空间中（即通过命名空间隔离），通常只能看到命名空间内的东西。chroot命令，不仅影响文件系统，还影响PID，网络接口等。如果你通过 --privileged 参数启动了一个docker容器，那么所涉及的命名空间隔离将获得额外的权限，比如创建更多子命名空间隔离的能力。这是完成docker-in-docker（即在docker中运行docker）的核心技巧。有关更多细节，Jérôme是这方面的专家，请在这里关注他的详细说明。
总之，这就是大致步骤：
 构建一个容器环境，完成docker，minikube，kubectl和依赖项的安装。
 添加一个假的systemctl shim来欺骗Minikube在没有真正安装systemd的环境中运行。
 使用 --privileged 参数启动容器
 让容器启动它自己内部的dockerd，这就是DinD的一部分。
 让容器通过参数 minikube --vm-driver = none 启动minikube，以便在容器中的minikube可以与与之一起运行的dockerd连接。
  所有你需要做的就是通过 docker run --privileged 运行容器，接着你就可以去使用kubectl了。这时如果你愿意，你可以在容器内运行kubectl，并得到一个真正的用完可随时删除的环境。
你现在可以试试它：
docker run --privileged --rm -it quay.io/aspenmesh/minikube-dind docker exec -it &amp;lt;container&amp;gt; /bin/bash # kubectl get nodes &amp;lt;....&amp;gt; # kubectl create -f https://k8s.</description>
    </item>
    
    <item>
      <title>Istio 0.8 发布了！</title>
      <link>https://servicemesher.github.io/blog/istio-0.8-release-note/</link>
      <pubDate>Fri, 01 Jun 2018 11:41:00 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/istio-0.8-release-note/</guid>
      <description>原文地址：Isito 0.8
译者：Jimmy Song
 北京时间 2018 年 6 月 1 日（儿童节）上午 9: 30 Istio 0.8.0 LTS（长期支持版本）发布。该版本除了常见的一堆错误修复和性能改进之外，还包含以下更新和新功能。
网络  改进了流量管理模型。我们终于准备好了推出新的流量管理配置模型。该模型增加了许多新功能并解决了先前模型的可用性问题。istioctl 中内置了一个转换工具来帮助您迁移旧模型。试用新的流量管理模型。 Ingress/Egress 网关。我们不再支持将 Kubernetes Ingress 配置与 Istio 路由规则相结合，因为这会导致一些错误和可靠性问题。Istio 现在支持独立于 Kubernetes 和 Cloud Foundry 平台的 ingress/egress 网关，并与路由规则无缝集成。 新的网关支持基于服务器名称指示（Server Name Indication）的路由，以及根据 SNI 值提供证书。HTTPS 访问外部服务将基于 SNI自动配置。 Envoy v2。用户可以选择使用 Envoy 的 v2 API 注入 sidecar。在这种模式下，Pilot使用 Envoy 的 v2 聚合发现服务 API将配置推送到数据平面。该方式提高了控制平面的可扩展性。 受限入站端口。我们现在将 Pod 中的入站端口限制为由该 Pod 内运行的应用程序所声明的端口。  安全  介绍 Citadel。我们终于给安全组件确定了名字。它就是我们之前称呼的 Istio-Auth 或 Istio-CA，现在我们将它称之为 Citadel。 多集群支持。对于多集群部署，支持在每集群中使用 Citadel，以便所有 Citade 都拥有相同的根证书且工作负载可以通过网格彼此验证。 验证策略。我们引入了可用于配置服务间认证策略身份认证（相互 TLS）和最终用户认证。这是启用相互 TLS 的推荐方式（通过现有的配置标志和服务注释）。了解更多。  遥测  自我报告。现在 Mixer 和 Pilot 产生的流量也会通过 Isitio 的遥测管道，就像网格中的其他服务一样。  部署  上碟 Istio 小菜。Istio 有丰富的功能，但是用户不一定要全部安装和使用。通过使用 Helm 或 istioctl gen-deploy，用户可以选择安装他们想要的功能。例如，用户可能只想安装 Pilot 并享受流量管理功能，无需处理 Mixer 或 Citadel。详细了解通过 Helm 定制和 istioctl gen-deploy。  Mixer 适配器  CloudWatch。Mixer 现在可以向 AWS CloudWatch 报告指标。了解更多。  0.</description>
    </item>
    
    <item>
      <title>Istio 的 GitOps——像代码一样管理 Istio 配置</title>
      <link>https://servicemesher.github.io/blog/gitops-for-istio-manage-istio-config-like-code/</link>
      <pubDate>Thu, 31 May 2018 21:12:03 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/gitops-for-istio-manage-istio-config-like-code/</guid>
      <description>译者：Jimmy Song
原文地址：GitOps for Istio - Manage Istio Config like Code
 在今年的哥本哈根 Kubecon 大会上，Weaveworks 的 CEO Alexis Richardson 与 Varun Talwar（来自一家隐形创业公司）谈到了 GitOps 工作流程和 Istio。会后 Weaveworks 的 Stefan Prodan 进行了的演示，介绍如何使用 GitOps 上线和管理 Istio 的金丝雀部署。
会谈和演示中解释了：
 什么是 GitOps？为什么需要它？ Istio 和 GitOps 的最佳实践是如何管理在其上运行的应用程序的。 如何使用 GitOps 工作流程和 Istio 进行金丝雀部署。  什么是GitOps？ GitOps 是实现持续交付的一种方式。“GitOps 使用 Git 作为声明式基础架构和应用程序的真实来源” Alexis Richardson 说。
当对 Git 进行更改时，自动化交付管道会上线对基础架构的更改。但是这个想法还可以更进一步——使用工具来比较实际的生产状态和源代码控制中描述的状态，然后告诉你什么时候集群的状态跟描述的不符。
Git 启用声明式工具 通过使用 Git 这样的声明式工具可以对整套配置文件做版本控制。通过将 Git 作为唯一的配置来源，可以很方便的复制整套基础架构，从而将系统的平均恢复时间从几小时缩短到几分钟。
GitOps 赋能开发人员拥抱运维 Weave Cloud 的 GitOps 核心机制在于 CI/CD 工具，其关键是支持 Git 集群同步的持续部署（CD）和发布管理。Weave Cloud 部署专为版本控制系统和声明式应用程序堆栈而设计。以往开发人员都是使用 Git 管理代码和提交 PR（Pull Request），现在他们也可以使用 Git 来加速和简化 Kubernetes 和 Istio 等其他声明式技术的运维工作。</description>
    </item>
    
  </channel>
</rss>