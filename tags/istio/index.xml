<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Istio on ServiceMesher</title>
    <link>https://servicemesher.github.io/tags/istio/</link>
    <description>Recent content in Istio on ServiceMesher</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh</language>
    <lastBuildDate>Mon, 04 Jun 2018 16:09:57 +0800</lastBuildDate>
    
	<atom:link href="https://servicemesher.github.io/tags/istio/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Istio 0.8 的 Helm Chart 解析</title>
      <link>https://servicemesher.github.io/blog/helm-chart-for-istio-0.8/</link>
      <pubDate>Mon, 04 Jun 2018 16:09:57 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/helm-chart-for-istio-0.8/</guid>
      <description>儿童节期间，拖拉了一个多月的 Istio 0.8 正式发布了，这可能是 Istio 1.0 之前的最后一个 LTS 版本，意义重大。
新版本中，原来的 Kubernetes 安装文件 install/kubernetes/istio.yaml，变成了 install/kubernetes/istio-demo.yaml，是的，你没看错，这个 LTS 的安装文件名字叫 demo。查看了一下文档，大概察觉到不靠谱的 Istio 发布组的意图了：这个项目的组件相对比较复杂，原有的一些选项是靠 ConfigMap 以及 istioctl 分别调整的，现在通过重新设计的 Helm Chart，安装选项用 values.yml 或者 helm 命令行的方式来进行集中管理了。下面就由看看 Istio 的 Helm Chart 的安装部署及其结构。
使用 Helm 安装 Istio 安装包内的 Helm 目录中包含了 Istio 的 Chart，官方提供了两种方法：
 用 Helm 生成 istio.yaml，然后自行安装。 用 Tiller 直接安装。  很明显，两种方法并没有什么本质区别。例如第一个命令：
helm template install/kubernetes/helm/istio \ --name istio --namespace \ istio-system &amp;gt; $HOME/istio.yaml  这里说的是使用 install/kubernetes/helm/istio 目录中的 Chart 进行渲染，生成的内容保存到 $HOME/istio.</description>
    </item>
    
    <item>
      <title>使用 Minikube-in-a-Container 和 Jenkins 构建 Istio</title>
      <link>https://servicemesher.github.io/blog/building-istio-with-minikube-in-a-container-and-jenkins/</link>
      <pubDate>Mon, 04 Jun 2018 11:33:16 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/building-istio-with-minikube-in-a-container-and-jenkins/</guid>
      <description>原文链接：https://blog.aspenmesh.io/blog/2018/01/building-istio-with-minikube-in-a-container-and-jenkins/
作者：Andrew Jenkins
译者：戴佳顺
 AspenMesh提供一种Istio的分布式架构支持，这意味着即使与上游Istio项目无关，我们也需要能够测试和修复Bug。为此我们已开发构建了我们自己的打包和测试基础架构方案。如果你对Istio的CI（持续集成）也感兴趣，请参考我们已经投入使用，可能有用但还没有提交给Circle CI或GKE的组件。
这篇文章描述的是我们如何制作一个新的Minikube-in-a-Container容器和使用Jenkins Pipeline来构建和测试Istio的流程脚本。如果你觉得有必要，你可以通过docker run上运行minikube容器，然后在容器中部署功能性的kubernetes集群，不需要使用时可随时删除。Jenkins bits现在可帮助你构建Istio，也可以作为初始环境，以便在容器内构建容器。
Minikube-in-a-container 这部分描述了我们如何构建一个可以在构建过程中用来运行Istio冒烟测试的Minikube-in-a-container镜像。我们最初不是这么想的，我们最初使用本地localkube环境。我们不能让它在特定环境外工作，我们认为这是由于localkube和minikube之间有一点差异导致的。所以这是一个作为我们修复它使它能正常工作的记录。我们还添加了一些额外选项和工具，以便在生成的容器中使用Istio。这没有什么太多花样，但如果你要做类似的事情，我们希望它给你启发。
Minikube可能对你来说是一个可以在随身携带的笔记本上通过虚机运行自己kubernetes集群的非常熟悉的项目。这种方法非常方便，但在某些情况下（比如不提供嵌套虚拟化的云提供商），你就不能或者不希望基于虚机来完成了。由于docker现在可以运行在docker内部，我们决定尝试在docker容器内制作我们自己的kubernetes集群。一个非持久性的kubernetes容器很容易启动，也可进行一些测试，并在完成后进行删除。同时这也非常适合持续集成。
在我们的模型方案中，Kubernetes集群创建子docker容器（而不是Jérôme Petazzoni所提到的兄弟容器方案）。我们是故意这样做的，宁愿隔离子容器，而不是共享Docker构建的缓存。但是你应该在将你应用改造为DinD（docker in docker）之前阅读Jérôme的文章，也许DooD（在docker out of docker）是对你而言更好的方案。这篇文章供你参考。我们避免架构“变得更坏”的同时，对看起来“坏”和“丑”部分也应进行避免。
当你启动docker容器时，会要求docker在OS内核中创建和设置一些命名空间（namespaces），然后在通过这些命名空间启动你的容器。命名空间像一个沙箱：当你在命名空间中（即通过命名空间隔离），通常只能看到命名空间内的东西。chroot命令，不仅影响文件系统，还影响PID，网络接口等。如果你通过 --privileged 参数启动了一个docker容器，那么所涉及的命名空间隔离将获得额外的权限，比如创建更多子命名空间隔离的能力。这是完成docker-in-docker（即在docker中运行docker）的核心技巧。有关更多细节，Jérôme是这方面的专家，请在这里关注他的详细说明。
总之，这就是大致步骤：
 构建一个容器环境，完成docker，minikube，kubectl和依赖项的安装。
 添加一个假的systemctl shim来欺骗Minikube在没有真正安装systemd的环境中运行。
 使用 --privileged 参数启动容器
 让容器启动它自己内部的dockerd，这就是DinD的一部分。
 让容器通过参数 minikube --vm-driver = none 启动minikube，以便在容器中的minikube可以与与之一起运行的dockerd连接。
  所有你需要做的就是通过 docker run --privileged 运行容器，接着你就可以去使用kubectl了。这时如果你愿意，你可以在容器内运行kubectl，并得到一个真正的用完可随时删除的环境。
你现在可以试试它：
docker run --privileged --rm -it quay.io/aspenmesh/minikube-dind docker exec -it &amp;lt;container&amp;gt; /bin/bash # kubectl get nodes &amp;lt;....&amp;gt; # kubectl create -f https://k8s.</description>
    </item>
    
    <item>
      <title>Istio 0.8 发布了！</title>
      <link>https://servicemesher.github.io/blog/istio-0.8-release-note/</link>
      <pubDate>Fri, 01 Jun 2018 11:41:00 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/istio-0.8-release-note/</guid>
      <description>原文地址：Isito 0.8
译者：Jimmy Song
 北京时间 2018 年 6 月 1 日（儿童节）上午 9: 30 Istio 0.8.0 LTS（长期支持版本）发布。该版本除了常见的一堆错误修复和性能改进之外，还包含以下更新和新功能。
网络  改进了流量管理模型。我们终于准备好了推出新的流量管理配置模型。该模型增加了许多新功能并解决了先前模型的可用性问题。istioctl 中内置了一个转换工具来帮助您迁移旧模型。试用新的流量管理模型。 Ingress/Egress 网关。我们不再支持将 Kubernetes Ingress 配置与 Istio 路由规则相结合，因为这会导致一些错误和可靠性问题。Istio 现在支持独立于 Kubernetes 和 Cloud Foundry 平台的 ingress/egress 网关，并与路由规则无缝集成。 新的网关支持基于服务器名称指示（Server Name Indication）的路由，以及根据 SNI 值提供证书。HTTPS 访问外部服务将基于 SNI自动配置。 Envoy v2。用户可以选择使用 Envoy 的 v2 API 注入 sidecar。在这种模式下，Pilot使用 Envoy 的 v2 聚合发现服务 API将配置推送到数据平面。该方式提高了控制平面的可扩展性。 受限入站端口。我们现在将 Pod 中的入站端口限制为由该 Pod 内运行的应用程序所声明的端口。  安全  介绍 Citadel。我们终于给安全组件确定了名字。它就是我们之前称呼的 Istio-Auth 或 Istio-CA，现在我们将它称之为 Citadel。 多集群支持。对于多集群部署，支持在每集群中使用 Citadel，以便所有 Citade 都拥有相同的根证书且工作负载可以通过网格彼此验证。 验证策略。我们引入了可用于配置服务间认证策略身份认证（相互 TLS）和最终用户认证。这是启用相互 TLS 的推荐方式（通过现有的配置标志和服务注释）。了解更多。  遥测  自我报告。现在 Mixer 和 Pilot 产生的流量也会通过 Isitio 的遥测管道，就像网格中的其他服务一样。  部署  上碟 Istio 小菜。Istio 有丰富的功能，但是用户不一定要全部安装和使用。通过使用 Helm 或 istioctl gen-deploy，用户可以选择安装他们想要的功能。例如，用户可能只想安装 Pilot 并享受流量管理功能，无需处理 Mixer 或 Citadel。详细了解通过 Helm 定制和 istioctl gen-deploy。  Mixer 适配器  CloudWatch。Mixer 现在可以向 AWS CloudWatch 报告指标。了解更多。  0.</description>
    </item>
    
    <item>
      <title>Istio 的 GitOps——像代码一样管理 Istio 配置</title>
      <link>https://servicemesher.github.io/blog/gitops-for-istio-manage-istio-config-like-code/</link>
      <pubDate>Thu, 31 May 2018 21:12:03 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/gitops-for-istio-manage-istio-config-like-code/</guid>
      <description>译者：Jimmy Song
原文地址：GitOps for Istio - Manage Istio Config like Code
 在今年的哥本哈根 Kubecon 大会上，Weaveworks 的 CEO Alexis Richardson 与 Varun Talwar（来自一家隐形创业公司）谈到了 GitOps 工作流程和 Istio。会后 Weaveworks 的 Stefan Prodan 进行了的演示，介绍如何使用 GitOps 上线和管理 Istio 的金丝雀部署。
会谈和演示中解释了：
 什么是 GitOps？为什么需要它？ Istio 和 GitOps 的最佳实践是如何管理在其上运行的应用程序的。 如何使用 GitOps 工作流程和 Istio 进行金丝雀部署。  什么是GitOps？ GitOps 是实现持续交付的一种方式。“GitOps 使用 Git 作为声明式基础架构和应用程序的真实来源” Alexis Richardson 说。
当对 Git 进行更改时，自动化交付管道会上线对基础架构的更改。但是这个想法还可以更进一步——使用工具来比较实际的生产状态和源代码控制中描述的状态，然后告诉你什么时候集群的状态跟描述的不符。
Git 启用声明式工具 通过使用 Git 这样的声明式工具可以对整套配置文件做版本控制。通过将 Git 作为唯一的配置来源，可以很方便的复制整套基础架构，从而将系统的平均恢复时间从几小时缩短到几分钟。
GitOps 赋能开发人员拥抱运维 Weave Cloud 的 GitOps 核心机制在于 CI/CD 工具，其关键是支持 Git 集群同步的持续部署（CD）和发布管理。Weave Cloud 部署专为版本控制系统和声明式应用程序堆栈而设计。以往开发人员都是使用 Git 管理代码和提交 PR（Pull Request），现在他们也可以使用 Git 来加速和简化 Kubernetes 和 Istio 等其他声明式技术的运维工作。</description>
    </item>
    
    <item>
      <title>深入解读Service Mesh背后的技术细节</title>
      <link>https://servicemesher.github.io/blog/deepin-service-mesh-tech-details/</link>
      <pubDate>Wed, 23 May 2018 16:09:57 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/deepin-service-mesh-tech-details/</guid>
      <description>在Kubernetes称为容器编排的标准之后，Service Mesh开始火了起来，但是很多文章讲概念的多，讲技术细节的少，所以专门写一篇文章，来解析Service Mesh背后的技术细节。
原文地址：刘超的通俗云计算
一、Service Mesh是Kubernetes支撑微服务能力拼图的最后一块
在上一篇文章为什么 kubernetes 天然适合微服务中我们提到，Kubernetes是一个奇葩所在，他的组件复杂，概念复杂，在没有实施微服务之前，你可能会觉得为什么Kubernetes要设计的这么复杂，但是一旦你要实施微服务，你会发现Kubernetes中的所有概念，都是有用的。
在我们微服务设计的是个要点中，我们会发现Kubernetes都能够有相应的组件和概念，提供相应的支持。
其中最后的一块拼图就是服务发现，与熔断限流降级。
众所周知，Kubernetes的服务发现是通过Service来实现的，服务之间的转发是通过kube-proxy下发iptables规则来实现的，这个只能实现最基本的服务发现和转发能力，不能满足高并发应用下的高级的服务特性，比较SpringCloud和Dubbo有一定的差距，于是Service Mesh诞生了，他期望讲熔断，限流，降级等特性，从应用层，下沉到基础设施层去实现，从而使得Kubernetes和容器全面接管微服务。
二、以Istio为例讲述Service Mesh中的技术关键点
就如SDN一样，Service Mesh将服务请求的转发分为控制面和数据面，因而分析他，也是从数据面先分析转发的能力，然后再分析控制面如何下发命令。今天这篇文章重点讲述两个组件Envoy和Pilot
一切从Envoy开始
我们首先来看，如果没有融入Service Mesh，Envoy本身能够做什么事情呢？
Envoy是一个高性能的C++写的proxy转发器，那Envoy如何转发请求呢？需要定一些规则，然后按照这些规则进行转发。
规则可以是静态的，放在配置文件中的，启动的时候加载，要想重新加载，一般需要重新启动，但是Envoy支持热加载和热重启，一定程度上缓解了这个问题。
当然最好的方式是规则设置为动态的，放在统一的地方维护，这个统一的地方在Envoy眼中看来称为Discovery Service，过一段时间去这里拿一下配置，就修改了转发策略。
无论是静态的，还是动态的，在配置里面往往会配置四个东西。
一是listener，也即envoy既然是proxy，专门做转发，就得监听一个端口，接入请求，然后才能够根据策略转发，这个监听的端口称为listener
二是endpoint，是目标的ip地址和端口，这个是proxy最终将请求转发到的地方。
三是cluster，一个cluster是具有完全相同行为的多个endpoint，也即如果有三个容器在运行，就会有三个IP和端口，但是部署的是完全相同的三个服务，他们组成一个Cluster，从cluster到endpoint的过程称为负载均衡，可以轮询等。
四是route，有时候多个cluster具有类似的功能，但是是不同的版本号，可以通过route规则，选择将请求路由到某一个版本号，也即某一个cluster。
这四个的静态配置的例子如下：
如图所示，listener被配置为监听本地127.0.0.1的10000接口，route配置为某个url的前缀转发到哪个cluster，cluster里面配置负载均衡策略，hosts里面是所有的endpoint。
如果你想简单的将envoy使用起来，不用什么service mesh，一个进程，加上这个配置文件，就可以了，就能够转发请求了。
对于动态配置，也应该配置发现中心，也即Discovery Service，对于上述四种配置，各对应相应的DS，所以有LDS, RDS, CDS, EDS。
动态配置的例子如下：
控制面Pilot的工作模式
数据面envoy可以通过加装静态配置文件的方式运行，而动态信息，需要从Discovery Service去拿。
Discovery Service就是部署在控制面的，在istio中，是Pilot。
如图为Pilot的架构，最下面一层是envoy的API，就是提供Discovery Service的API，这个API的规则由envoy定，但是不是Pilot调用Envoy，而是Envoy去主动调用Pilot的这个API。
Pilot最上面一层称为Platform Adapter，这一层是干什么的呢？这一层不是Kubernetes, Mesos调用Pilot，而是Pilot通过调用Kubernetes来发现服务之间的关系。
这是理解Istio比较绕的一个点。也即pilot使用Kubernetes的Service，仅仅使用它的服务发现功能，而不使用它的转发功能，pilot通过在kubernetes里面注册一个controller来监听事件，从而获取Service和Kubernetes的Endpoint以及Pod的关系，但是在转发层面，就不会再使用kube-proxy根据service下发的iptables规则进行转发了，而是将这些映射关系转换成为pilot自己的转发模型，下发到envoy进行转发，envoy不会使用kube-proxy的那些iptables规则。这样就把控制面和数据面彻底分离开来，服务之间的相互关系是管理面的事情，不要和真正的转发绑定在一起，而是绕到pilot后方。
Pilot另外一个对外的接口是Rules API，这是给管理员的接口，管理员通过这个接口设定一些规则，这些规则往往是应用于Routes, Clusters, Endpoints的，而都有哪些Clusters和Endpoints，是由Platform Adapter这面通过服务发现得到的。
自动发现的这些Clusters和Endpoints，外加管理员设置的规则，形成了Pilot的数据模型，其实就是他自己定义的一系列数据结构，然后通过envoy API暴露出去，等待envoy去拉取这些规则。
常见的一种人工规则是Routes，通过服务发现，Pilot可以从Kubernetes那里知道Service B有两个版本，一般是两个Deployment，属于同一个Service，管理员通过调用Pilot的Rules API，来设置两个版本之间的Route规则，一个占99%的流量，一个占1%的流量，这两方面信息形成Pilot的数据结构模型，然后通过Envoy API下发，Envoy就会根据这个规则设置转发策略了。
另一个常用的场景就是负载均衡，Pilot通过Kubernetes的Service发现Service B包含一个Deployment，但是有三个副本，于是通过Envoy API下发规则，使得Envoy在这三个副本之间进行负载均衡，而非通过Kubernetes本身Service的负载均衡机制。
三、以Istio为例解析Service Mesh的技术细节
了解了Service Mesh的大概原理，接下来我们通过一个例子来解析其中的技术细节。
凡是试验过Istio的同学都应该尝试过下面这个BookInfo的例子，不很复杂，但是麻雀虽小五脏俱全。
在这个例子中，我们重点关注ProductPage这个服务，对Reviews服务的调用，这里涉及到路由策略和负载均衡。</description>
    </item>
    
    <item>
      <title>Istio Service Mesh 教程</title>
      <link>https://servicemesher.github.io/blog/istio-service-mesh-tutorial/</link>
      <pubDate>Tue, 22 May 2018 12:16:22 +0800</pubDate>
      
      <guid>https://servicemesher.github.io/blog/istio-service-mesh-tutorial/</guid>
      <description>本文是 Istio 管理 Java 微服务的案例教程，使用的所有工具和软件全部基于开源方案，替换了 redhat-developer-demos/istio-tutorial 中的 minishift 环境，使用 kubernetes-vagrant-centos-cluster 替代，沿用了原有的微服务示例，使用 Zipkin 做分布式追踪而不是 Jaeger。
本文中的代码和 YAML 文件见 https://github.com/rootsongjc/istio-tutorial。
原文地址：https://jimmysong.io/posts/istio-tutorial/
准备环境 在进行本教程前需要先准备以下工具和环境。
 8G 以上内存 Vagrant 2.0+ Virtualbox 5.0 + 提前下载 kubernetes1.9.1 的 release 压缩包 docker 1.12+ kubectl 1.9.1+ maven 3.5.2+ istioctl 0.7.1 git curl、gzip、tar kubetail siege  安装 Kubernetes 请参考 kubernetes-vagrant-centos-cluster 在本地启动拥有三个节点的 kubernetes 集群。
git clone https://github.com/rootsongjc/kubernetes-vagrant-centos-cluster.git cd kubernetes-vagrant-centos-cluster vagrant up  安装 Istio 在 kubernetes-vagrant-centos-cluster 中的包含 Istio 0.7.1 的安装 YAML 文件，运行下面的命令安装 Istio。</description>
    </item>
    
  </channel>
</rss>